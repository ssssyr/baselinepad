# Default Hyperparameters

# ======================================================
#               General Training Settings
# ======================================================
training:
  # Paths
  feature_path: "features" # Path to training data features
  video_path: null # Path to video data (for co-training)
  results_dir: "results" # Directory to save checkpoints and logs

  # Model
  model: "DiT-XL/2" # Model architecture
  image_size: 256 # Image size (must be divisible by 8)
  num_classes: 1000 # Number of classes for classifier-free guidance
  predict_horizon: 1 # Number of future frames to predict
  skip_step: 4 # Steps to skip between frames

  # Training Loop
  epochs: 1400 # Total training epochs
  global_batch_size: 256 # Total batch size across all GPUs
  global_seed: 0 # Random seed for reproducibility
  num_workers: 4 # Number of data loader workers
  without_ema: false # Disable Exponential Moving Average of model weights

  # Logging and Checkpointing
  log_every: 100 # Log training status every N steps
  eval_every: 5000 # Evaluate model every N steps
  ckpt_every: 30000 # Save checkpoint every N steps
  ckpt_wrapper: false # (Legacy) Wrapper for saving memory

  # Resuming Training
  resume: null # Path to a specific checkpoint to resume from
  auto_resume: false # Automatically resume from the latest checkpoint in results_dir

# ======================================================
#                 Component Settings
# ======================================================
components:
  # VAE (Variational Autoencoder)
  vae: "ema" # VAE model type (ema or mse)

  # Initialization
  dit_init: null # Path to pretrained DiT weights for initialization
  rgb_init: null # Path to pretrained model for RGB components

  # Attention Mechanism
  attn_mask: false # Use attention mask

  # Text Conditioning
  text_cond: false # Enable text conditioning
  clip_path: "/home/syr/code/models/clip-vit-base-patch32" # Path to CLIP model
  text_emb_size: 512 # Dimension of text embeddings

  # Depth Conditioning
  use_depth: false # Enable depth conditioning
  d_hidden_size: 32 # Hidden size for depth encoder
  d_patch_size: 8 # Patch size for depth encoder
  depth_filter: false # Apply a filter to depth images

  # Action Conditioning
  action_steps: 0 # Number of action steps to predict/condition on
  action_dim: 7 # Dimension of the action space
  action_scale: 10.0 # Scaling factor for actions
  absolute_action: false # Use absolute actions instead of relative
  action_condition: false # Condition on the current action/pose
  learnable_action_pos: false # Use learnable positional embeddings for actions

  # Loss Configuration
  action_loss_lambda: 1.0 # Weight for the action prediction loss
  action_loss_start: 50000 # Step to start applying action loss

# ======================================================
#                  MoE Settings
# ======================================================
moe:
  use_moe: false # Enable Mixture of Experts
  num_experts: 8 # Number of experts
  moe_top_k: 2 # Number of experts to route to for each token
  aux_loss_weight: 0.01 # Weight for the auxiliary load-balancing loss
  router_z_loss_weight: 0.001 # Weight for the router z-loss to prevent logit explosion

# ======================================================
#                  WandB Logging
# ======================================================
wandb:
  use_wandb: false # Enable Weights & Biases logging
  wandb_project: "prediction_with_action" # Wandb project name
  wandb_run_name: null # Wandb run name (defaults to model-timestamp)

