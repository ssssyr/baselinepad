#!/bin/bash

# =================================================================
#   MetaWorld Action (Pose) Prediction Training - 4x A100
#   Project root, config, and script paths use ABSOLUTE paths.
# =================================================================

set -e

# ---- 0) é¡¹ç›®æ ¹ç›®å½•ï¼ˆæŒ‰ä½ çš„å®é™…è·¯å¾„ï¼‰ ----
SCRIPT_DIR="/home/ct_24210860031/812code/SYR/baselinepad"
CONFIG_FILE="$SCRIPT_DIR/configs/metaworld_4d.yaml"
TRAIN_SCRIPT="$SCRIPT_DIR/train_robot.py"

# ---- 1) æ‰“å°åŸºç¡€ä¿¡æ¯ ----
echo "ğŸš€ Starting MetaWorld Action Prediction Training on 4x A100..."
echo "ğŸ“ Project: $SCRIPT_DIR"
echo "ğŸ“ Config:  $CONFIG_FILE"
echo "ğŸ–¥ï¸  GPUs:    4,5,6,7 (A100)"

# ---- 2) æ•°æ®ä¸ç»“æœç›®å½• ----
FEATURE_PATH="/home/ct_24210860031/812datasets/SYR/metaworld_features"  # æå–åçš„ç‰¹å¾ç›®å½•ï¼ˆå« dataset_rgb_s_d.jsonï¼‰
TIMESTAMP="$(date +%Y%m%d_%H%M%S)"
RESULTS_DIR="$SCRIPT_DIR/results/metaworld_a100_${TIMESTAMP}"

echo "ğŸ“ Data Path:    $FEATURE_PATH"
echo "ğŸ’¾ Results Dir:  $RESULTS_DIR"
mkdir -p "$RESULTS_DIR"

# ---- 3) GPU/ç«¯å£/æ‰¹æ¬¡è®¾ç½® ----
export CUDA_VISIBLE_DEVICES=4,5,6,7
NUM_GPUS=4
MASTER_PORT=${PORT:-$(shuf -i 29500-49151 -n 1)}
echo "ğŸ”Œ Master Port:  $MASTER_PORT"

# æ¯å¡ batch ä¸å…¨å±€ batchï¼ˆä¼ ç»™ --global-batch-sizeï¼›è„šæœ¬å†…ä¼šæŒ‰ GPU æ•°åˆ‡åˆ†ï¼‰
PER_GPU_BATCH_SIZE=16
TOTAL_BATCH_SIZE=$((PER_GPU_BATCH_SIZE * NUM_GPUS))
echo "ğŸ“¦ Batch Size:   $PER_GPU_BATCH_SIZE per GPU â†’ $TOTAL_BATCH_SIZE total"

# ---- 4) è®­ç»ƒè¶…å‚ï¼ˆYAML æ–‡ä»¶ä¼˜å…ˆï¼‰----
# ä¸‹é¢çš„å‚æ•°å·²æ³¨é‡Šæ‰ï¼Œå°†ä½¿ç”¨ config æ–‡ä»¶ä¸­çš„è®¾ç½®ã€‚
# å¦‚æœéœ€è¦ä»è„šæœ¬æŒ‡å®šï¼Œè¯·å–æ¶ˆæ³¨é‡Šå¹¶æ·»åŠ åˆ°ä¸‹é¢çš„ torchrun å‘½ä»¤ä¸­ã€‚
# EPOCHS=1000
# LEARNING_RATE=1e-5

# ---- 4.5) Checkpoint æ¢å¤è®¾ç½® ----
CHECKPOINT_PATH=""
echo ""
read -p "ğŸ”„ Do you want to resume from checkpoint? (y/n): " RESUME_CHOICE
if [[ "$RESUME_CHOICE" =~ ^[Yy]$ ]]; then
    echo "ğŸ“ Please enter the full path to your checkpoint file:"
    echo "   Example: /home/ct_24210860031/812code/SYR/baselinepad/results/metaworld_a100_20251119_014323/000-DiT-XL-2-2025-11-19-01-43-46/checkpoints/0020000.pt"
    read -p "ğŸ¯ Checkpoint path: " CHECKPOINT_PATH
    
    if [ ! -f "$CHECKPOINT_PATH" ]; then
        echo "âŒ ERROR: Checkpoint file '$CHECKPOINT_PATH' not found!"
        exit 1
    fi
    
    echo "âœ… Found checkpoint: $CHECKPOINT_PATH"
    echo "ğŸ”„ Training will resume from this checkpoint..."
else
    echo "ğŸ†• Starting fresh training (no checkpoint resume)"
fi

# ---- 5) W&B é…ç½®ï¼ˆæŒ‰éœ€å¼€å¯ï¼‰ ----
WANDB_PROJECT="metaworld-action-prediction"
WANDB_RUN_NAME="4xA100-metaworld-bs${TOTAL_BATCH_SIZE}-${TIMESTAMP}"
echo "ğŸ“Š WandB:       $WANDB_PROJECT / $WANDB_RUN_NAME"

# ---- 6) ç³»ç»Ÿç¯å¢ƒä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰----
export TORCH_CUDNN_V8_API_ENABLED=1
export NCCL_DEBUG=WARN

echo "================================================================"
echo "ğŸ¯ Training Configuration Summary:"
echo "   â€¢ Script: $TRAIN_SCRIPT"
echo "   â€¢ Config: $CONFIG_FILE"
echo "   â€¢ GPUs:   $CUDA_VISIBLE_DEVICES ($NUM_GPUS cards)"
echo "   â€¢ Epochs: (from config)"
echo "   â€¢ LR:     (from config)"
echo "   â€¢ Global Batch Size: $TOTAL_BATCH_SIZE"
echo "   â€¢ Results: $RESULTS_DIR"
echo "   â€¢ WandB:  $WANDB_RUN_NAME"
echo "================================================================"

# ---- 7) é¢„æ£€ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰----
echo "ğŸ” Pre-flight checks..."

if [ ! -f "$CONFIG_FILE" ]; then
  echo "âŒ ERROR: Config file '$CONFIG_FILE' not found!"
  exit 1
fi

if [ ! -f "$TRAIN_SCRIPT" ]; then
  echo "âŒ ERROR: Training script '$TRAIN_SCRIPT' not found!"
  exit 1
fi

if [ ! -d "$FEATURE_PATH" ]; then
  echo "âŒ ERROR: Feature data directory '$FEATURE_PATH' not found!"
  exit 1
fi

if [ ! -f "$FEATURE_PATH/dataset_rgb_s_d.json" ]; then
  echo "âŒ ERROR: '$FEATURE_PATH/dataset_rgb_s_d.json' not found!"
  exit 1
fi

echo "âœ… All checks passed!"

# ---- 8) è¿›å…¥é¡¹ç›®ç›®å½•å¹¶è®¾ç½® PYTHONPATH ----
echo "ğŸ“ Changing to script directory: $SCRIPT_DIR"
cd "$SCRIPT_DIR" || { echo "âŒ ERROR: Cannot change to $SCRIPT_DIR"; exit 1; }
export PYTHONPATH="$SCRIPT_DIR:$PYTHONPATH"
echo "ğŸ Python path set to: $PYTHONPATH"

# ---- 9) å¯åŠ¨è®­ç»ƒï¼ˆå…³é”®ï¼šæ˜¾å¼ä¼ å…¥ --config ä¸ --feature-pathï¼‰----
echo "ğŸš€ Launching training..."
echo "Command:"
echo "torchrun --nproc_per_node=$NUM_GPUS --master_port=$MASTER_PORT $TRAIN_SCRIPT \\"
echo "  --config \"$CONFIG_FILE\" \\"
echo "  --feature-path \"$FEATURE_PATH\" \\"
echo "  --global-batch-size \"$TOTAL_BATCH_SIZE\" \\"

echo "  --results-dir \"$RESULTS_DIR\" \\"
echo "  --use-wandb \\"
echo "  --wandb-project \"$WANDB_PROJECT\" \\"
echo "  --wandb-run-name \"$WANDB_RUN_NAME\" \\"
echo "  --dynamics"

torchrun --nproc_per_node=$NUM_GPUS --master_port=$MASTER_PORT "$TRAIN_SCRIPT" \
  --config "$CONFIG_FILE" \
  --feature-path "$FEATURE_PATH" \
  --global-batch-size "$TOTAL_BATCH_SIZE" \

  --results-dir "$RESULTS_DIR" \
  --use-wandb \
  --wandb-project "$WANDB_PROJECT" \
  --wandb-run-name "$WANDB_RUN_NAME" \
  --dynamics \
  ${CHECKPOINT_PATH:+--resume "$CHECKPOINT_PATH"}

# ---- 10) ç»“æŸçŠ¶æ€ ----
EXIT_CODE=$?
echo "================================================================"
if [ $EXIT_CODE -eq 0 ]; then
  echo "ğŸ‰ Training completed successfully!"
  echo "ğŸ“ Results saved to: $RESULTS_DIR"
else
  echo "âŒ Training failed with exit code: $EXIT_CODE"
  echo "ğŸ” Check the logs above for error details"
fi
echo "================================================================"

exit $EXIT_CODE
